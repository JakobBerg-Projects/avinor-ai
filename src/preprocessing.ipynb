{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e03d5c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c52389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "no_holidays = holidays.NO()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28e9c7",
   "metadata": {},
   "source": [
    "# Parametre & stier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88b899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/raw_data/historical_flights.csv\"\n",
    "PREDICTION_PATH=\"../data/raw_data/scheduled_october2025.csv\"\n",
    "CUTOFF = \"2024-01-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cdaa0",
   "metadata": {},
   "source": [
    "# Hjelpefunksjoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d761742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flights(path: str, prediction: bool) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if prediction:\n",
    "        for col in [\"std\", \"sta\"]:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    else:\n",
    "        for col in [\"std\", \"sta\", \"atd\", \"ata\"]:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def handle_wrong_times(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    n0 = len(df)\n",
    "\n",
    "    # 1) Planlagt varighet\n",
    "    df['duration'] = df['sta'] - df['std']\n",
    "    df1 = df[(df['duration'] >= pd.Timedelta(0)) &\n",
    "             (df['duration'] <= pd.Timedelta(hours=10))].copy()\n",
    "    n1 = len(df1)\n",
    "\n",
    "    # 2) Ekstremt tidlige avvik (delay < -500 min)\n",
    "    dep_too_early = df1[\"atd\"].notna() & ((df1[\"atd\"] - df1[\"std\"]) < pd.Timedelta(minutes=-500))\n",
    "    arr_too_early = df1[\"ata\"].notna() & ((df1[\"ata\"] - df1[\"sta\"]) < pd.Timedelta(minutes=-500))\n",
    "    df2 = df1.loc[~(dep_too_early | arr_too_early)].copy()\n",
    "    n2 = len(df2)\n",
    "\n",
    "    print(f\"Totalt: {n0}\")\n",
    "    print(f\"  Fjernet på varighet: {n0 - n1}\")\n",
    "    print(f\"  Fjernet på ekstremt tidlig dep/arr: {n1 - n2}\")\n",
    "    print(f\"Beholdt: {n2}\")\n",
    "\n",
    "    return df2\n",
    "\n",
    "def build_full_grid(df: pd.DataFrame, prediction:bool) -> pd.DataFrame:\n",
    "    groups = pd.concat([df[\"dep_airport_group\"], df[\"arr_airport_group\"]]).dropna().unique()\n",
    "    if prediction:\n",
    "        tmin = pd.to_datetime(df[[\"std\",\"sta\"]].min(numeric_only=False).min()).floor(\"h\")\n",
    "        tmax = pd.to_datetime(df[[\"std\",\"sta\"]].max(numeric_only=False).max()).ceil(\"h\")\n",
    "    else:\n",
    "        tmin = pd.to_datetime(df[[\"std\",\"sta\",\"atd\",\"ata\"]].min(numeric_only=False).min()).floor(\"h\")\n",
    "        tmax = pd.to_datetime(df[[\"std\",\"sta\",\"atd\",\"ata\"]].max(numeric_only=False).max()).ceil(\"h\")\n",
    "    all_hours = pd.date_range(tmin, tmax, freq=\"h\")\n",
    "    return pd.MultiIndex.from_product([groups, all_hours], names=[\"airport_group\",\"timestamp\"]).to_frame(index=False)\n",
    "\n",
    "\n",
    "def make_intervals(df: pd.DataFrame, actual: bool = True) -> pd.DataFrame:\n",
    "\n",
    "    if actual:\n",
    "        dep = df.dropna(subset=[\"atd\"]).copy()\n",
    "        dep[\"start\"] = dep[\"atd\"] - pd.to_timedelta(15, \"m\")\n",
    "        dep[\"end\"]   = dep[\"atd\"] + pd.to_timedelta(8, \"m\")\n",
    "\n",
    "\n",
    "        arr = df.dropna(subset=[\"ata\"]).copy()\n",
    "        arr[\"start\"] = arr[\"ata\"] - pd.to_timedelta(16, \"m\")\n",
    "        arr[\"end\"]   = arr[\"ata\"] + pd.to_timedelta(5, \"m\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        dep = df.dropna(subset=[\"std\"]).copy()\n",
    "        dep[\"start\"] = dep[\"std\"] - pd.to_timedelta(15, \"m\")\n",
    "        dep[\"end\"]   = dep[\"std\"] + pd.to_timedelta(8, \"m\")\n",
    "\n",
    "        arr = df.dropna(subset=[\"sta\"]).copy()\n",
    "        arr[\"start\"] = arr[\"sta\"] - pd.to_timedelta(16, \"m\")\n",
    "        arr[\"end\"]   = arr[\"sta\"] + pd.to_timedelta(5, \"m\")\n",
    "\n",
    "\n",
    "    dep[\"airport_group\"] = dep[\"dep_airport_group\"]\n",
    "    dep[\"type\"] = \"departure\"\n",
    "    arr[\"airport_group\"] = arr[\"arr_airport_group\"]\n",
    "    arr[\"type\"] = \"arrival\"\n",
    "\n",
    "    intervals = pd.concat([dep, arr], ignore_index=True)\n",
    "    intervals = intervals.dropna(subset=[\"airport_group\"])\n",
    "    return intervals\n",
    "\n",
    "\n",
    "def expand_to_hours(intervals: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, row in intervals.iterrows():\n",
    "        hour_start = row[\"start\"].floor(\"h\")\n",
    "        hour_end = row[\"end\"].floor(\"h\")\n",
    "        hours = pd.date_range(hour_start, hour_end, freq=\"h\")\n",
    "        for h in hours:\n",
    "            rows.append({**row, \"timestamp\": h})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def hourly_overlap_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    hour = group[\"timestamp\"].iloc[0]\n",
    "    airport = group[\"airport_group\"].iloc[0]\n",
    "    events = []\n",
    "    for _, row in group.iterrows():\n",
    "        events.append((row[\"start\"], +1))\n",
    "        events.append((row[\"end\"], -1))\n",
    "    events.sort()\n",
    "    active, overlap = 0, 0\n",
    "    for _, change in events:\n",
    "        active += change\n",
    "        if active > 1:\n",
    "            overlap = 1\n",
    "            break\n",
    "    return pd.DataFrame([{\"airport_group\": airport, \"timestamp\": hour, \"target\": overlap}])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_hourly_features_from(intervals_any: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature-aggregater, men generisk (kan bruke actual ELLER scheduled intervaller).\n",
    "    \"\"\"\n",
    "    df = intervals_any.copy()\n",
    "    df[\"duration_min\"] = ((df[\"sta\"] - df[\"std\"]).dt.total_seconds() / 60)\n",
    "    \n",
    "    if \"flight_id\" in df:\n",
    "        df[\"airline\"] = df[\"flight_id\"].astype(str).str.extract(r\"^([A-Za-z]+)\")\n",
    "    df[\"airline\"] = \"\"\n",
    "\n",
    "    feats = df.groupby([\"airport_group\", \"timestamp\"]).agg(\n",
    "        flights_cnt     = (\"flight_id\", \"count\"),\n",
    "        avg_duration    = (\"duration_min\", \"mean\"),\n",
    "        max_duration    = (\"duration_min\", \"max\"),\n",
    "        passenger_share = (\"service_type\", lambda x: (x == \"J\").mean()),\n",
    "        cargo_share     = (\"service_type\", lambda x: (x == \"P\").mean()),\n",
    "        charter_share   = (\"service_type\", lambda x: (x == \"C\").mean()),\n",
    "        airline = (\"airline\", lambda x: x.mode().iloc[0] if not x.mode().empty else \"\")\n",
    "    ).reset_index()\n",
    "\n",
    "    return feats\n",
    "def add_time_features(feats: pd.DataFrame) -> pd.DataFrame:\n",
    "    feats[\"dow\"]     = feats[\"timestamp\"].dt.dayofweek\n",
    "    feats[\"holiday\"] = feats[\"timestamp\"].apply(lambda x: x.date() in no_holidays)\n",
    "    feats[\"month\"]   = feats[\"timestamp\"].dt.month\n",
    "    feats[\"hournum\"] = feats[\"timestamp\"].dt.hour\n",
    "    feats[\"weekend\"] = (feats[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    feats[\"date\"] = feats[\"timestamp\"].dt.normalize()\n",
    "\n",
    "    feats[\"daily_flights_cnt\"] = feats.groupby(\n",
    "        [\"airport_group\", \"date\"]\n",
    "    )[\"flights_cnt\"].transform(\"sum\")\n",
    "\n",
    "    feats = feats.sort_values([\"airport_group\", \"timestamp\"])\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def add_prev_next(feats: pd.DataFrame) -> pd.DataFrame:\n",
    "    feats = feats.sort_values([\"airport_group\", \"timestamp\"])\n",
    "    feats[\"flights_cnt_prev\"] = feats.groupby(\"airport_group\")[\"flights_cnt\"].shift(1)\n",
    "    feats[\"flights_cnt_next\"] = feats.groupby(\"airport_group\")[\"flights_cnt\"].shift(-1)\n",
    "    feats[[\"flights_cnt_prev\", \"flights_cnt_next\"]] = feats[[\"flights_cnt_prev\", \"flights_cnt_next\"]].fillna(0).astype(int)\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5bbc7c",
   "metadata": {},
   "source": [
    "# Last rådata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4c8c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_id</th>\n",
       "      <th>dep_airport</th>\n",
       "      <th>dep_airport_group</th>\n",
       "      <th>arr_airport</th>\n",
       "      <th>arr_airport_group</th>\n",
       "      <th>service_type</th>\n",
       "      <th>std</th>\n",
       "      <th>sta</th>\n",
       "      <th>cancelled</th>\n",
       "      <th>atd</th>\n",
       "      <th>ata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WF149</td>\n",
       "      <td>HOV</td>\n",
       "      <td>B</td>\n",
       "      <td>OSL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J</td>\n",
       "      <td>2018-01-02 16:40:00</td>\n",
       "      <td>2018-01-02 17:15:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-01-02 18:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WF722</td>\n",
       "      <td>OSL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MJF</td>\n",
       "      <td>D</td>\n",
       "      <td>J</td>\n",
       "      <td>2018-01-28 13:04:00</td>\n",
       "      <td>2018-01-28 14:50:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WF188</td>\n",
       "      <td>FDE</td>\n",
       "      <td>A</td>\n",
       "      <td>OSL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J</td>\n",
       "      <td>2018-04-07 07:10:00</td>\n",
       "      <td>2018-04-07 08:10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-04-07 07:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WF176</td>\n",
       "      <td>HOV</td>\n",
       "      <td>B</td>\n",
       "      <td>OSL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J</td>\n",
       "      <td>2018-04-07 11:00:00</td>\n",
       "      <td>2018-04-07 12:05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-04-07 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WF148</td>\n",
       "      <td>HOV</td>\n",
       "      <td>B</td>\n",
       "      <td>OSL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J</td>\n",
       "      <td>2018-04-30 08:25:00</td>\n",
       "      <td>2018-04-30 09:26:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-04-30 09:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410437</th>\n",
       "      <td>WF153</td>\n",
       "      <td>SOG</td>\n",
       "      <td>A</td>\n",
       "      <td>HOV</td>\n",
       "      <td>B</td>\n",
       "      <td>J</td>\n",
       "      <td>2025-05-03 09:25:00</td>\n",
       "      <td>2025-05-03 10:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03 09:47:00</td>\n",
       "      <td>2025-05-03 10:19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410438</th>\n",
       "      <td>WF153</td>\n",
       "      <td>BGO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOG</td>\n",
       "      <td>A</td>\n",
       "      <td>J</td>\n",
       "      <td>2025-05-03 08:35:00</td>\n",
       "      <td>2025-05-03 09:10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03 08:29:00</td>\n",
       "      <td>2025-05-03 09:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410439</th>\n",
       "      <td>WF158</td>\n",
       "      <td>OSL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOV</td>\n",
       "      <td>B</td>\n",
       "      <td>J</td>\n",
       "      <td>2025-05-03 14:40:00</td>\n",
       "      <td>2025-05-03 15:50:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03 14:35:00</td>\n",
       "      <td>2025-05-03 15:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410440</th>\n",
       "      <td>WF721</td>\n",
       "      <td>SSJ</td>\n",
       "      <td>D</td>\n",
       "      <td>TRD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J</td>\n",
       "      <td>2025-05-03 08:50:00</td>\n",
       "      <td>2025-05-03 09:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03 08:49:00</td>\n",
       "      <td>2025-05-03 09:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410441</th>\n",
       "      <td>WF182</td>\n",
       "      <td>FDE</td>\n",
       "      <td>A</td>\n",
       "      <td>OSL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J</td>\n",
       "      <td>2025-05-03 05:10:00</td>\n",
       "      <td>2025-05-03 06:15:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03 05:08:00</td>\n",
       "      <td>2025-05-03 06:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>410442 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       flight_id dep_airport dep_airport_group arr_airport arr_airport_group  \\\n",
       "0          WF149         HOV                 B         OSL               NaN   \n",
       "1          WF722         OSL               NaN         MJF                 D   \n",
       "2          WF188         FDE                 A         OSL               NaN   \n",
       "3          WF176         HOV                 B         OSL               NaN   \n",
       "4          WF148         HOV                 B         OSL               NaN   \n",
       "...          ...         ...               ...         ...               ...   \n",
       "410437     WF153         SOG                 A         HOV                 B   \n",
       "410438     WF153         BGO               NaN         SOG                 A   \n",
       "410439     WF158         OSL               NaN         HOV                 B   \n",
       "410440     WF721         SSJ                 D         TRD               NaN   \n",
       "410441     WF182         FDE                 A         OSL               NaN   \n",
       "\n",
       "       service_type                 std                 sta  cancelled  \\\n",
       "0                 J 2018-01-02 16:40:00 2018-01-02 17:15:00          0   \n",
       "1                 J 2018-01-28 13:04:00 2018-01-28 14:50:00          0   \n",
       "2                 J 2018-04-07 07:10:00 2018-04-07 08:10:00          0   \n",
       "3                 J 2018-04-07 11:00:00 2018-04-07 12:05:00          0   \n",
       "4                 J 2018-04-30 08:25:00 2018-04-30 09:26:00          0   \n",
       "...             ...                 ...                 ...        ...   \n",
       "410437            J 2025-05-03 09:25:00 2025-05-03 10:00:00          0   \n",
       "410438            J 2025-05-03 08:35:00 2025-05-03 09:10:00          0   \n",
       "410439            J 2025-05-03 14:40:00 2025-05-03 15:50:00          0   \n",
       "410440            J 2025-05-03 08:50:00 2025-05-03 09:45:00          0   \n",
       "410441            J 2025-05-03 05:10:00 2025-05-03 06:15:00          0   \n",
       "\n",
       "                       atd                 ata  \n",
       "0                      NaT 2018-01-02 18:53:00  \n",
       "1                      NaT                 NaT  \n",
       "2                      NaT 2018-04-07 07:55:00  \n",
       "3                      NaT 2018-04-07 12:00:00  \n",
       "4                      NaT 2018-04-30 09:36:00  \n",
       "...                    ...                 ...  \n",
       "410437 2025-05-03 09:47:00 2025-05-03 10:19:00  \n",
       "410438 2025-05-03 08:29:00 2025-05-03 09:18:00  \n",
       "410439 2025-05-03 14:35:00 2025-05-03 15:39:00  \n",
       "410440 2025-05-03 08:49:00 2025-05-03 09:39:00  \n",
       "410441 2025-05-03 05:08:00 2025-05-03 06:00:00  \n",
       "\n",
       "[410442 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = load_flights(DATA_PATH, prediction=False)\n",
    "df_pred = load_flights(PREDICTION_PATH, prediction=True)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ce544",
   "metadata": {},
   "source": [
    "# Rens og filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f02755bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totalt: 410442\n",
      "  Fjernet på varighet: 86\n",
      "  Fjernet på ekstremt tidlig dep/arr: 97\n",
      "Beholdt: 410259\n"
     ]
    }
   ],
   "source": [
    "df = handle_wrong_times(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f25d5d",
   "metadata": {},
   "source": [
    "# Bygger full grid for alle timer i intervallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc7f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = build_full_grid(df, prediction=False)\n",
    "grid_pred=build_full_grid(df_pred, prediction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873f406",
   "metadata": {},
   "source": [
    "# Intervaller (actual) + hourly overlap (actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4cd48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/ndm52t3d58j7bkp7_dx_ygc00000gn/T/ipykernel_14010/1819245093.py:8: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(hourly_overlap_group)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_group</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>target_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 12:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 13:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 14:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 16:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 17:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airport_group           timestamp  target_actual\n",
       "0             A 2018-01-01 12:00:00              0\n",
       "0             A 2018-01-01 13:00:00              1\n",
       "0             A 2018-01-01 14:00:00              0\n",
       "0             A 2018-01-01 16:00:00              1\n",
       "0             A 2018-01-01 17:00:00              1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervals_actual = make_intervals(df, actual=True)\n",
    "intervals_actual_expanded = expand_to_hours(intervals_actual)\n",
    "\n",
    "\n",
    "hourly_actual = (\n",
    "    intervals_actual_expanded\n",
    "    .groupby([\"airport_group\", \"timestamp\"], group_keys=False)\n",
    "    .apply(hourly_overlap_group)\n",
    "    .rename(columns={\"target\": \"target_actual\"})\n",
    ")\n",
    "\n",
    "hourly_actual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796bc6bc",
   "metadata": {},
   "source": [
    "# Intervaller (scheduled) + hourly overlap (scheduled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bece8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/ndm52t3d58j7bkp7_dx_ygc00000gn/T/ipykernel_14010/3526365751.py:7: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(hourly_overlap_group)\n"
     ]
    }
   ],
   "source": [
    "intervals_sched = make_intervals(df, actual=False)\n",
    "intervals_sched_expanded = expand_to_hours(intervals_sched)\n",
    "\n",
    "hourly_sched = (\n",
    "    intervals_sched_expanded\n",
    "    .groupby([\"airport_group\", \"timestamp\"], group_keys=False)\n",
    "    .apply(hourly_overlap_group)\n",
    "    .rename(columns={\"target\": \"target_sched\"})\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa52cd",
   "metadata": {},
   "source": [
    "# Prediction Data Intervaller (scheduled) + hourly overlap (scheduled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc4c34a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/ndm52t3d58j7bkp7_dx_ygc00000gn/T/ipykernel_14010/2247729525.py:7: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(hourly_overlap_group)\n"
     ]
    }
   ],
   "source": [
    "intervals_pred = make_intervals(df_pred, actual=False)\n",
    "intervals_pred_expanded = expand_to_hours(intervals_pred)\n",
    "\n",
    "hourly_pred=(\n",
    "    intervals_pred_expanded\n",
    "    .groupby([\"airport_group\", \"timestamp\"], group_keys=False)\n",
    "    .apply(hourly_overlap_group)\n",
    "    .rename(columns={\"target\": \"target_sched\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2d5dd",
   "metadata": {},
   "source": [
    "# Merge targets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f8e67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_group</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>target_actual</th>\n",
       "      <th>target_sched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>2018-01-01 07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>2018-01-01 09:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>2018-01-01 10:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>2018-01-01 11:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airport_group           timestamp  target_actual  target_sched\n",
       "0             B 2018-01-01 07:00:00              0             0\n",
       "1             B 2018-01-01 08:00:00              0             0\n",
       "2             B 2018-01-01 09:00:00              0             0\n",
       "3             B 2018-01-01 10:00:00              0             1\n",
       "4             B 2018-01-01 11:00:00              1             1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly = (grid\n",
    "          .merge(hourly_actual, on=[\"airport_group\",\"timestamp\"], how=\"left\")\n",
    "          .merge(hourly_sched,  on=[\"airport_group\",\"timestamp\"], how=\"left\"))\n",
    "hourly[[\"target_actual\",\"target_sched\"]] = hourly[[\"target_actual\",\"target_sched\"]].fillna(0).astype(int)\n",
    "hourly.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa66744",
   "metadata": {},
   "source": [
    "# Merge targets prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b6ba346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_group</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>target_sched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G</td>\n",
       "      <td>2025-10-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G</td>\n",
       "      <td>2025-10-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G</td>\n",
       "      <td>2025-10-01 05:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G</td>\n",
       "      <td>2025-10-01 06:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2025-10-01 07:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airport_group           timestamp  target_sched\n",
       "0             G 2025-10-01 03:00:00             1\n",
       "1             G 2025-10-01 04:00:00             0\n",
       "2             G 2025-10-01 05:00:00             0\n",
       "3             G 2025-10-01 06:00:00             1\n",
       "4             G 2025-10-01 07:00:00             0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_pred=grid_pred.merge(hourly_pred, on=[\"airport_group\",\"timestamp\"], how=\"left\")\n",
    "hourly_pred[[\"target_sched\"]] = hourly_pred[[\"target_sched\"]].fillna(0).astype(int)\n",
    "hourly_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa986abf",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794ae0e",
   "metadata": {},
   "source": [
    "# Feature-agg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec75601d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['airline'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m feat_raw = \u001b[43mmake_hourly_features_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintervals_sched_expanded\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m feat_raw.head()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mmake_hourly_features_from\u001b[39m\u001b[34m(intervals_any)\u001b[39m\n\u001b[32m    114\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mduration_min\u001b[39m\u001b[33m\"\u001b[39m] = ((df[\u001b[33m\"\u001b[39m\u001b[33msta\u001b[39m\u001b[33m\"\u001b[39m] - df[\u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m]).dt.total_seconds() / \u001b[32m60\u001b[39m)\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m##if \"flight_id\" in df:\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m##df[\"airline\"] = df[\"flight_id\"].astype(str).str.extract(r\"^([A-Za-z]+)\")\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m###df[\"airline\"] = \"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m feats = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mairport_group\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflights_cnt\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflight_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mavg_duration\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mduration_min\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_duration\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mduration_min\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassenger_share\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mJ\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcargo_share\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mP\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcharter_share\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mairline\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mairline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.reset_index()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m feats\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/groupby/generic.py:1432\u001b[39m, in \u001b[36mDataFrameGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = engine_kwargs\n\u001b[32m   1431\u001b[39m op = GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args=args, kwargs=kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m result = \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1434\u001b[39m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[32m   1435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.as_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/apply.py:190\u001b[39m, in \u001b[36mApply.agg\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_str()\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agg_list_like()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/apply.py:423\u001b[39m, in \u001b[36mApply.agg_dict_like\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame | Series:\n\u001b[32m    416\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m \u001b[33;03m    Result of aggregation.\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/apply.py:1603\u001b[39m, in \u001b[36mGroupByApply.agg_or_apply_dict_like\u001b[39m\u001b[34m(self, op_name)\u001b[39m\n\u001b[32m   1598\u001b[39m     kwargs.update({\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m: engine, \u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m: engine_kwargs})\n\u001b[32m   1600\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m com.temp_setattr(\n\u001b[32m   1601\u001b[39m     obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition=\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1602\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1603\u001b[39m     result_index, result_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1606\u001b[39m result = \u001b[38;5;28mself\u001b[39m.wrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[32m   1607\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/apply.py:462\u001b[39m, in \u001b[36mApply.compute_dict_like\u001b[39m\u001b[34m(self, op_name, selected_obj, selection, kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m is_groupby = \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[32m    461\u001b[39m func = cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m.func)\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m func = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m is_non_unique_col = (\n\u001b[32m    465\u001b[39m     selected_obj.ndim == \u001b[32m2\u001b[39m\n\u001b[32m    466\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj.columns.nunique() < \u001b[38;5;28mlen\u001b[39m(selected_obj.columns)\n\u001b[32m    467\u001b[39m )\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m selected_obj.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/apply.py:663\u001b[39m, in \u001b[36mApply.normalize_dictlike_arg\u001b[39m\u001b[34m(self, how, obj, func)\u001b[39m\n\u001b[32m    661\u001b[39m     cols = Index(\u001b[38;5;28mlist\u001b[39m(func.keys())).difference(obj.columns, sort=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m do not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    665\u001b[39m aggregator_types = (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    667\u001b[39m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: \"Column(s) ['airline'] do not exist\""
     ]
    }
   ],
   "source": [
    "feat_raw = make_hourly_features_from(intervals_sched_expanded.copy())\n",
    "feat_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2cb32",
   "metadata": {},
   "source": [
    "# Feature-agg pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25fdbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_group</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flights_cnt</th>\n",
       "      <th>avg_duration</th>\n",
       "      <th>max_duration</th>\n",
       "      <th>passenger_share</th>\n",
       "      <th>cargo_share</th>\n",
       "      <th>charter_share</th>\n",
       "      <th>airline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2025-10-01 03:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2025-10-01 04:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>40.833333</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>2025-10-01 06:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>51.666667</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2025-10-01 07:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2025-10-01 08:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airport_group           timestamp  flights_cnt  avg_duration  max_duration  \\\n",
       "0             A 2025-10-01 03:00:00            2     47.500000          65.0   \n",
       "1             A 2025-10-01 04:00:00            6     40.833333          65.0   \n",
       "2             A 2025-10-01 06:00:00            3     51.666667          65.0   \n",
       "3             A 2025-10-01 07:00:00            6     41.666667          65.0   \n",
       "4             A 2025-10-01 08:00:00            2     47.500000          65.0   \n",
       "\n",
       "   passenger_share  cargo_share  charter_share airline  \n",
       "0              1.0          0.0            0.0      WF  \n",
       "1              1.0          0.0            0.0      WF  \n",
       "2              1.0          0.0            0.0      WF  \n",
       "3              1.0          0.0            0.0      WF  \n",
       "4              1.0          0.0            0.0      WF  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_pred_raw = make_hourly_features_from(intervals_pred_expanded.copy())\n",
    "feat_pred_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86863108",
   "metadata": {},
   "source": [
    "## Merger med full grid for alle timer og fyller inn verdier for timer der det ikke er noen fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_full = grid.merge(feat_raw, on=[\"airport_group\",\"timestamp\"], how=\"left\")\n",
    "feat_full_pred = grid_pred.merge(feat_pred_raw, on=[\"airport_group\",\"timestamp\"], how=\"left\")\n",
    "\n",
    "for df_ in (feat_full, feat_full_pred):\n",
    "    for c in [\"flights_cnt\",\"avg_duration\",\"max_duration\",\n",
    "              \"passenger_share\",\"cargo_share\",\"charter_share\",\"airline\", \"hour\"]:\n",
    "        if c in df_.columns:\n",
    "            if c == \"flights_cnt\":\n",
    "                df_[c] = df_[c].fillna(0).astype(int)\n",
    "            elif c == \"airline\":\n",
    "                df_[c] = df_[c].fillna(\"\")\n",
    "            else:\n",
    "                df_[c] = df_[c].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a38290",
   "metadata": {},
   "source": [
    "## Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_full      = add_time_features(feat_full)\n",
    "feat_full_pred = add_time_features(feat_full_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34a47a",
   "metadata": {},
   "source": [
    "## Next and previous hour flights count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54097b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_full     = add_prev_next(feat_full)\n",
    "feat_full_pred = add_prev_next(feat_full_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2335d",
   "metadata": {},
   "source": [
    "## Vær"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c277ec",
   "metadata": {},
   "source": [
    "#### Add locations for airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60080842",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_locations_df = pd.read_csv(\"../data/raw_data/airports.csv\")\n",
    "airport_groups_df = pd.read_csv(\"../data/raw_data/airportgroups.csv\")\n",
    "\n",
    "airport_locations_df = airport_locations_df.merge(airport_groups_df, left_on=\"iata_code\", right_on=\"airport\")\n",
    "airport_locations_df = airport_locations_df.groupby(by = \"airport_group\")[[\"latitude_deg\", \"longitude_deg\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0770050a",
   "metadata": {},
   "source": [
    "#### Find weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: USE AIRPORT LOCATION ABOVE\n",
    "sensors = {}\n",
    "\n",
    "for (group, location) in airport_locations_df.iterrows():\n",
    "    endpoint = \"https://frost.met.no/sources/v0.jsonld\"\n",
    "    parameters = {\n",
    "        \"types\": \"SensorSystem\",\n",
    "        \"geometry\": f\"nearest(POINT({location[\"longitude_deg\"]} {location[\"latitude_deg\"]}))\"\n",
    "    }\n",
    "    r = requests.get(endpoint, parameters, auth=(os.getenv(\"FROST_ID\"),''))\n",
    "    resp = r.json()\n",
    "\n",
    "    sensors[group] = resp\n",
    "\n",
    "for key in sensors.keys():\n",
    "    sensors[key] = sorted(sensors[key][\"data\"], key=lambda v: v[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fcf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_group</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flights_cnt</th>\n",
       "      <th>avg_duration</th>\n",
       "      <th>max_duration</th>\n",
       "      <th>passenger_share</th>\n",
       "      <th>cargo_share</th>\n",
       "      <th>charter_share</th>\n",
       "      <th>airline</th>\n",
       "      <th>dow</th>\n",
       "      <th>holiday</th>\n",
       "      <th>month</th>\n",
       "      <th>hournum</th>\n",
       "      <th>weekend</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_flights_cnt</th>\n",
       "      <th>flights_cnt_prev</th>\n",
       "      <th>flights_cnt_next</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66449</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66450</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66451</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 09:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66452</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 10:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66453</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 11:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WF</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66454</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 12:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WF</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66455</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 13:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66456</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 14:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66457</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 15:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WF</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66458</th>\n",
       "      <td>A</td>\n",
       "      <td>2018-01-01 16:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>46.250000</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WF</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      airport_group           timestamp  flights_cnt  avg_duration  \\\n",
       "66449             A 2018-01-01 07:00:00            0      0.000000   \n",
       "66450             A 2018-01-01 08:00:00            0      0.000000   \n",
       "66451             A 2018-01-01 09:00:00            0      0.000000   \n",
       "66452             A 2018-01-01 10:00:00            0      0.000000   \n",
       "66453             A 2018-01-01 11:00:00            3     56.666667   \n",
       "66454             A 2018-01-01 12:00:00            3     46.666667   \n",
       "66455             A 2018-01-01 13:00:00            0      0.000000   \n",
       "66456             A 2018-01-01 14:00:00            0      0.000000   \n",
       "66457             A 2018-01-01 15:00:00            2     60.000000   \n",
       "66458             A 2018-01-01 16:00:00            4     46.250000   \n",
       "\n",
       "       max_duration  passenger_share  cargo_share  charter_share airline  dow  \\\n",
       "66449           0.0              0.0          0.0            0.0            0   \n",
       "66450           0.0              0.0          0.0            0.0            0   \n",
       "66451           0.0              0.0          0.0            0.0            0   \n",
       "66452           0.0              0.0          0.0            0.0            0   \n",
       "66453          60.0              1.0          0.0            0.0      WF    0   \n",
       "66454          60.0              1.0          0.0            0.0      WF    0   \n",
       "66455           0.0              0.0          0.0            0.0            0   \n",
       "66456           0.0              0.0          0.0            0.0            0   \n",
       "66457          60.0              1.0          0.0            0.0      WF    0   \n",
       "66458          65.0              1.0          0.0            0.0      WF    0   \n",
       "\n",
       "       holiday  month  hournum  weekend       date  daily_flights_cnt  \\\n",
       "66449     True      1        7        0 2018-01-01                 25   \n",
       "66450     True      1        8        0 2018-01-01                 25   \n",
       "66451     True      1        9        0 2018-01-01                 25   \n",
       "66452     True      1       10        0 2018-01-01                 25   \n",
       "66453     True      1       11        0 2018-01-01                 25   \n",
       "66454     True      1       12        0 2018-01-01                 25   \n",
       "66455     True      1       13        0 2018-01-01                 25   \n",
       "66456     True      1       14        0 2018-01-01                 25   \n",
       "66457     True      1       15        0 2018-01-01                 25   \n",
       "66458     True      1       16        0 2018-01-01                 25   \n",
       "\n",
       "       flights_cnt_prev  flights_cnt_next weather  \n",
       "66449                 0                 0    None  \n",
       "66450                 0                 0    None  \n",
       "66451                 0                 0    None  \n",
       "66452                 0                 3    None  \n",
       "66453                 0                 3    None  \n",
       "66454                 3                 0    None  \n",
       "66455                 3                 0    None  \n",
       "66456                 0                 2    None  \n",
       "66457                 0                 4    None  \n",
       "66458                 2                 4    None  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = {}\n",
    "values = [None] * 465143\n",
    "\n",
    "for (i, row) in feat_full.iterrows():\n",
    "    resp = None\n",
    "\n",
    "    if (row[\"airport_group\"] != \"A\"):\n",
    "        continue\n",
    "\n",
    "    if weather.get(row[\"timestamp\"].date()) != None:\n",
    "        resp = weather.get(row[\"timestamp\"].date())\n",
    "    else:\n",
    "        endpoint = 'https://frost.met.no/observations/v0.jsonld'\n",
    "        parameters = {\n",
    "            'sources': 'SN18700,SN90450',\n",
    "            'elements': 'mean(air_temperature P1D),sum(precipitation_amount P1D),mean(wind_speed P1D)',\n",
    "            'referencetime': f'{row[\"hour\"].date()}',\n",
    "        }\n",
    "        r = requests.get(endpoint, parameters, auth=(os.getenv(\"FROST_ID\"),''))\n",
    "    \n",
    "        if r.status_code == 200:\n",
    "            json = r.json()\n",
    "    \n",
    "            k = json['data'][0]['observations'][0]['elementId']\n",
    "            v = json['data'][0]['observations'][0]['value']\n",
    "            \n",
    "            resp = v\n",
    "            weather[row[\"hour\"].date()] = v\n",
    "        else:\n",
    "            resp = float('nan')\n",
    "            weather[row[\"hour\"].date()] = float('nan')\n",
    "\n",
    "    values[i] = resp\n",
    "\n",
    "    print(f\"{i} / 37490\")\n",
    "\n",
    "feat_full[\"weather\"] = values\n",
    "feat_full.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc468e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_full.iloc[0][\"weather\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376beb9",
   "metadata": {},
   "source": [
    "# Samle datasett + split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eef58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((368039, 21), (97104, 21), (5026, 19))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = hourly.merge(feat_full, on=[\"airport_group\", \"timestamp\"], how=\"left\").sort_values(\"timestamp\")\n",
    "pred= hourly_pred.merge(feat_full_pred,on=[\"airport_group\",\"timestamp\"], how=\"left\").sort_values(\"timestamp\")\n",
    "train = dataset[dataset[\"timestamp\"] < CUTOFF]\n",
    "val   = dataset[dataset[\"timestamp\"] >= CUTOFF]\n",
    "\n",
    "train.shape, val.shape, pred.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49ef9f",
   "metadata": {},
   "source": [
    "# lagre mellomfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/processed_data/train.csv', index=False)\n",
    "val.to_csv('../data/processed_data/val.csv', index=False)\n",
    "pred.to_csv('../data/processed_data/predict_oct2025.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF161",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
