{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e03d5c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c52389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "\n",
    "no_holidays = holidays.NO()\n",
    "DATA_PATH = \"../data/raw_data/historical_flights.csv\"\n",
    "PREDICTION_PATH=\"../data/raw_data/scheduled_october2025.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cdaa0",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d761742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flights(path: str, prediction: bool) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if prediction:\n",
    "        for col in [\"std\", \"sta\"]:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    else:\n",
    "        for col in [\"std\", \"sta\", \"atd\", \"ata\"]:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_wrong_times(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    n0 = len(df)\n",
    "\n",
    "    # 1) Planlagt varighet\n",
    "    df['duration'] = df['sta'] - df['std']\n",
    "    df1 = df[(df['duration'] >= pd.Timedelta(0)) & (df['duration'] <= pd.Timedelta(hours=10))].copy()\n",
    "    n1 = len(df1)\n",
    "\n",
    "    # 2) Ekstremt tidlige avvik (delay < -500 min)\n",
    "    dep_too_early = df1[\"atd\"].notna() & ((df1[\"atd\"] - df1[\"std\"]) < pd.Timedelta(minutes=-500))\n",
    "    arr_too_early = df1[\"ata\"].notna() & ((df1[\"ata\"] - df1[\"sta\"]) < pd.Timedelta(minutes=-500))\n",
    "    df2 = df1.loc[~(dep_too_early | arr_too_early)].copy()\n",
    "    n2 = len(df2)\n",
    "\n",
    "    print(f\"Totalt: {n0}\")\n",
    "    print(f\"  Fjernet på varighet: {n0 - n1}\")\n",
    "    print(f\"  Fjernet på ekstremt tidlig dep/arr: {n1 - n2}\")\n",
    "    print(f\"Beholdt: {n2}\")\n",
    "\n",
    "    return df2\n",
    "\n",
    "def build_full_grid(df: pd.DataFrame, prediction:bool) -> pd.DataFrame:\n",
    "    groups = pd.concat([df[\"dep_airport_group\"], df[\"arr_airport_group\"]]).dropna().unique()\n",
    "    \n",
    "    if prediction:\n",
    "        tmin = pd.to_datetime(df[[\"std\",\"sta\"]].min(numeric_only=False).min()).floor(\"h\")\n",
    "        tmax = pd.to_datetime(df[[\"std\",\"sta\"]].max(numeric_only=False).max()).ceil(\"h\")\n",
    "    else:\n",
    "        tmin = pd.to_datetime(df[[\"std\",\"sta\",\"atd\",\"ata\"]].min(numeric_only=False).min()).floor(\"h\")\n",
    "        tmax = pd.to_datetime(df[[\"std\",\"sta\",\"atd\",\"ata\"]].max(numeric_only=False).max()).ceil(\"h\")\n",
    "\n",
    "    all_hours = pd.date_range(tmin, tmax, freq=\"h\")\n",
    "\n",
    "    return pd.MultiIndex.from_product([groups, all_hours], names=[\"airport_group\",\"timestamp\"]).to_frame(index=False)\n",
    "\n",
    "\n",
    "def make_intervals(df: pd.DataFrame, actual: bool = True) -> pd.DataFrame:\n",
    "    if actual:\n",
    "        dep = df.dropna(subset=[\"atd\"]).copy()\n",
    "        dep[\"start\"] = dep[\"atd\"] - pd.to_timedelta(15, \"m\")\n",
    "        dep[\"end\"]   = dep[\"atd\"] + pd.to_timedelta(8, \"m\")\n",
    "\n",
    "        arr = df.dropna(subset=[\"ata\"]).copy()\n",
    "        arr[\"start\"] = arr[\"ata\"] - pd.to_timedelta(16, \"m\")\n",
    "        arr[\"end\"]   = arr[\"ata\"] + pd.to_timedelta(5, \"m\")\n",
    "    else:\n",
    "        dep = df.dropna(subset=[\"std\"]).copy()\n",
    "        dep[\"start\"] = dep[\"std\"] - pd.to_timedelta(15, \"m\")\n",
    "        dep[\"end\"]   = dep[\"std\"] + pd.to_timedelta(8, \"m\")\n",
    "\n",
    "        arr = df.dropna(subset=[\"sta\"]).copy()\n",
    "        arr[\"start\"] = arr[\"sta\"] - pd.to_timedelta(16, \"m\")\n",
    "        arr[\"end\"]   = arr[\"sta\"] + pd.to_timedelta(5, \"m\")\n",
    "\n",
    "    dep[\"airport_group\"] = dep[\"dep_airport_group\"]\n",
    "    dep[\"type\"] = \"departure\"\n",
    "    arr[\"airport_group\"] = arr[\"arr_airport_group\"]\n",
    "    arr[\"type\"] = \"arrival\"\n",
    "\n",
    "    intervals = pd.concat([dep, arr], ignore_index=True)\n",
    "    intervals = intervals.dropna(subset=[\"airport_group\"])\n",
    "\n",
    "    return intervals\n",
    "\n",
    "\n",
    "def expand_to_hours(intervals: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, row in intervals.iterrows():\n",
    "        hour_start = row[\"start\"].floor(\"h\")\n",
    "        hour_end = row[\"end\"].floor(\"h\")\n",
    "        hours = pd.date_range(hour_start, hour_end, freq=\"h\")\n",
    "        for h in hours:\n",
    "            rows.append({**row, \"timestamp\": h})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def hourly_overlap_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    hour = group[\"timestamp\"].iloc[0]\n",
    "    airport = group[\"airport_group\"].iloc[0]\n",
    "    events = []\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        events.append((row[\"start\"], +1))\n",
    "        events.append((row[\"end\"], -1))\n",
    "\n",
    "    events.sort()\n",
    "    active, overlap = 0, 0\n",
    "\n",
    "    for _, change in events:\n",
    "        active += change\n",
    "        if active > 1:\n",
    "            overlap = 1\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame([{\"airport_group\": airport, \"timestamp\": hour, \"target\": overlap}])\n",
    "\n",
    "def make_hourly_features_from(intervals_any: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature-aggregater, men generisk (kan bruke actual ELLER scheduled intervaller).\n",
    "    \"\"\"\n",
    "\n",
    "    df = intervals_any.copy()\n",
    "    df[\"duration_min\"] = ((df[\"sta\"] - df[\"std\"]).dt.total_seconds() / 60)\n",
    "    \n",
    "    if \"flight_id\" in df:\n",
    "        df[\"airline\"] = df[\"flight_id\"].astype(str).str.extract(r\"^([A-Za-z]+)\")\n",
    "    else:\n",
    "        df[\"airline\"] = \"\"\n",
    "\n",
    "    feats = df.groupby([\"airport_group\", \"timestamp\"]).agg(\n",
    "        flights_cnt     = (\"flight_id\", \"count\"),\n",
    "        avg_duration    = (\"duration_min\", \"mean\"),\n",
    "        max_duration    = (\"duration_min\", \"max\"),\n",
    "        passenger_share = (\"service_type\", lambda x: (x == \"J\").mean()),\n",
    "        cargo_share     = (\"service_type\", lambda x: (x == \"P\").mean()),\n",
    "        charter_share   = (\"service_type\", lambda x: (x == \"C\").mean()),\n",
    "        airline = (\"airline\", lambda x: x.mode().iloc[0] if not x.mode().empty else \"\")\n",
    "    ).reset_index()\n",
    "\n",
    "    return feats\n",
    "\n",
    "def add_time_features(feats: pd.DataFrame) -> pd.DataFrame:\n",
    "    feats[\"dow\"]     = feats[\"timestamp\"].dt.dayofweek\n",
    "    feats[\"holiday\"] = feats[\"timestamp\"].apply(lambda x: x.date() in no_holidays)\n",
    "    feats[\"month\"]   = feats[\"timestamp\"].dt.month\n",
    "    feats[\"hournum\"] = feats[\"timestamp\"].dt.hour\n",
    "    feats[\"weekend\"] = (feats[\"dow\"] >= 5).astype(int)\n",
    "    feats[\"date\"] = feats[\"timestamp\"].dt.normalize()\n",
    "    feats[\"daily_flights_cnt\"] = feats.groupby([\"airport_group\", \"date\"])[\"flights_cnt\"].transform(\"sum\")\n",
    "    feats = feats.sort_values([\"airport_group\", \"timestamp\"])\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def add_prev_next(feats: pd.DataFrame) -> pd.DataFrame:\n",
    "    feats = feats.sort_values([\"airport_group\", \"timestamp\"])\n",
    "    feats[\"flights_cnt_prev\"] = feats.groupby(\"airport_group\")[\"flights_cnt\"].shift(1)\n",
    "    feats[\"flights_cnt_next\"] = feats.groupby(\"airport_group\")[\"flights_cnt\"].shift(-1)\n",
    "    feats[[\"flights_cnt_prev\", \"flights_cnt_next\"]] = feats[[\"flights_cnt_prev\", \"flights_cnt_next\"]].fillna(0).astype(int)\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5bbc7c",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = load_flights(DATA_PATH, prediction=False)\n",
    "df_pred = load_flights(PREDICTION_PATH, prediction=True)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ce544",
   "metadata": {},
   "source": [
    "# Clean and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02755bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handle_wrong_times(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f25d5d",
   "metadata": {},
   "source": [
    "# Building full grid for all hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = build_full_grid(df, prediction=False)\n",
    "grid_pred = build_full_grid(df_pred, prediction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873f406",
   "metadata": {},
   "source": [
    "# Find *actual* intervals and hourly overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_actual = make_intervals(df, actual=True)\n",
    "intervals_actual_expanded = expand_to_hours(intervals_actual)\n",
    "\n",
    "hourly_actual = (\n",
    "    intervals_actual_expanded\n",
    "    .groupby([\"airport_group\", \"timestamp\"], group_keys=False)\n",
    "    .apply(hourly_overlap_group)\n",
    "    .rename(columns={\"target\": \"target_actual\"})\n",
    ")\n",
    "\n",
    "hourly_actual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796bc6bc",
   "metadata": {},
   "source": [
    "# Find *scheduled* intervals and hourly overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bece8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_sched = make_intervals(df, actual=False)\n",
    "intervals_sched_expanded = expand_to_hours(intervals_sched)\n",
    "\n",
    "hourly_sched = (\n",
    "    intervals_sched_expanded\n",
    "    .groupby([\"airport_group\", \"timestamp\"], group_keys=False)\n",
    "    .apply(hourly_overlap_group)\n",
    "    .rename(columns={\"target\": \"target_sched\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa52cd",
   "metadata": {},
   "source": [
    "# Find *actual* + *scheduled* intervals and hourly overlap for prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_pred = make_intervals(df_pred, actual=False)\n",
    "intervals_pred_expanded = expand_to_hours(intervals_pred)\n",
    "\n",
    "hourly_pred=(\n",
    "    intervals_pred_expanded\n",
    "    .groupby([\"airport_group\", \"timestamp\"], group_keys=False)\n",
    "    .apply(hourly_overlap_group)\n",
    "    .rename(columns={\"target\": \"target_sched\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2d5dd",
   "metadata": {},
   "source": [
    "# Merge targets datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly = (grid\n",
    "    .merge(hourly_actual, on=[\"airport_group\",\"timestamp\"], how=\"left\")\n",
    "    .merge(hourly_sched,  on=[\"airport_group\",\"timestamp\"], how=\"left\"))\n",
    "hourly[[\"target_actual\",\"target_sched\"]] = hourly[[\"target_actual\",\"target_sched\"]].fillna(0).astype(int)\n",
    "hourly.head()\n",
    "\n",
    "hourly_pred=grid_pred.merge(hourly_pred, on=[\"airport_group\",\"timestamp\"], how=\"left\")\n",
    "hourly_pred[[\"target_sched\"]] = hourly_pred[[\"target_sched\"]].fillna(0).astype(int)\n",
    "hourly_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa986abf",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794ae0e",
   "metadata": {},
   "source": [
    "### Feature-agg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_raw = make_hourly_features_from(intervals_sched_expanded.copy())\n",
    "feat_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2cb32",
   "metadata": {},
   "source": [
    "### Feature-agg for prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25fdbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pred_raw = make_hourly_features_from(intervals_pred_expanded.copy())\n",
    "feat_pred_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86863108",
   "metadata": {},
   "source": [
    "### Merging with full grid for every hour, and fills in values for hours where there's no planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_full = grid.merge(feat_raw, on=[\"airport_group\",\"timestamp\"], how=\"left\")\n",
    "feat_full_pred = grid_pred.merge(feat_pred_raw, on=[\"airport_group\",\"timestamp\"], how=\"left\")\n",
    "\n",
    "for df_ in (feat_full, feat_full_pred):\n",
    "    for c in [\"flights_cnt\",\"avg_duration\",\"max_duration\",\n",
    "              \"passenger_share\",\"cargo_share\",\"charter_share\",\"airline\", \"hour\"]:\n",
    "        if c in df_.columns:\n",
    "            if c == \"flights_cnt\":\n",
    "                df_[c] = df_[c].fillna(0).astype(int)\n",
    "            elif c == \"airline\":\n",
    "                df_[c] = df_[c].fillna(\"\")\n",
    "            else:\n",
    "                df_[c] = df_[c].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a38290",
   "metadata": {},
   "source": [
    "### Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_full      = add_time_features(feat_full)\n",
    "feat_full_pred = add_time_features(feat_full_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34a47a",
   "metadata": {},
   "source": [
    "### Next and previous hour flights count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54097b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_full      = add_prev_next(feat_full)\n",
    "feat_full_pred = add_prev_next(feat_full_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2335d",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c277ec",
   "metadata": {},
   "source": [
    "#### Get latitude/longitude of airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60080842",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_locations_df = pd.read_csv(\"../data/raw_data/airports.csv\")\n",
    "airport_groups_df = pd.read_csv(\"../data/raw_data/airportgroups.csv\")\n",
    "\n",
    "airport_locations_df = airport_locations_df.merge(airport_groups_df, left_on=\"iata_code\", right_on=\"airport\")\n",
    "airport_locations_df = airport_locations_df.groupby(by = \"airport_group\")[[\"latitude_deg\", \"longitude_deg\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0770050a",
   "metadata": {},
   "source": [
    "#### Find relevant weather sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = {}\n",
    "\n",
    "for (group, location) in airport_locations_df.iterrows():\n",
    "    endpoint = \"https://frost.met.no/sources/v0.jsonld\"\n",
    "    parameters = {\n",
    "        \"types\": \"SensorSystem\",\n",
    "        \"geometry\": f\"nearest(POINT({location[\"longitude_deg\"]} {location[\"latitude_deg\"]}))\"\n",
    "    }\n",
    "    r = requests.get(endpoint, parameters, auth=(os.getenv(\"FROST_ID\"),''))\n",
    "    resp = r.json()\n",
    "\n",
    "    sensors[group] = resp\n",
    "\n",
    "for key in sensors.keys():\n",
    "    sensors[key] = sorted(sensors[key][\"data\"], key=lambda v: v[\"distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586acf8",
   "metadata": {},
   "source": [
    "### Find weather observations on said sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = {}\n",
    "elements = [\"mean(air_temperature P1D)\", \"sum(precipitation_amount P1D)\"]\n",
    "count = 0\n",
    "\n",
    "for (i, row) in feat_full.iterrows():\n",
    "    count += 1\n",
    "    print(f\"{count} of {feat_full.shape[0]}\")\n",
    "\n",
    "    timestamp = row[\"timestamp\"].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if weather.get(row[\"airport_group\"]) == None:\n",
    "        weather[row[\"airport_group\"]] = {}\n",
    "    if weather[row[\"airport_group\"]].get(timestamp) == None:\n",
    "        weather[row[\"airport_group\"]][timestamp] = {}\n",
    "\n",
    "    if len(weather[row[\"airport_group\"]][timestamp].keys()) == 0:\n",
    "        endpoint = 'https://frost.met.no/observations/v0.jsonld'\n",
    "        parameters = {\n",
    "            'sources': sensors[row[\"airport_group\"]][0][\"id\"],\n",
    "            'elements': \",\".join(elements),\n",
    "            'referencetime': timestamp,\n",
    "        }\n",
    "        r = requests.get(endpoint, parameters, auth=(os.getenv(\"FROST_ID\"),''), timeout=10)\n",
    "    \n",
    "        if r.status_code == 200:\n",
    "            json = r.json()\n",
    "\n",
    "            for (k, v) in list(map(lambda v: (v[\"elementId\"], v[\"value\"]), filter(lambda v: v[\"timeOffset\"] == \"PT6H\", json[\"data\"][0][\"observations\"]))):\n",
    "                weather[row[\"airport_group\"]][timestamp][k] = float(v)\n",
    "        else:\n",
    "            for k in elements:\n",
    "                weather[row[\"airport_group\"]][timestamp][k] = float('nan')\n",
    "    \n",
    "    for observations in weather[row[\"airport_group\"]].values():\n",
    "        for (observation_k, observation_v) in observations.items():\n",
    "            feat_full.at[i, observation_k] = observation_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bff48",
   "metadata": {},
   "source": [
    "### Find weather forecast on said sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = {}\n",
    "\n",
    "for airport_group in set(feat_full_pred[\"airport_group\"]):\n",
    "    weather[airport_group] = {}\n",
    "\n",
    "    endpoint = \"https://api.met.no/weatherapi/subseasonal/1.0/complete\"\n",
    "    parameters = {\n",
    "        \"lat\": sensors[airport_group][0][\"geometry\"][\"coordinates\"][1],\n",
    "        \"lon\": sensors[airport_group][0][\"geometry\"][\"coordinates\"][0]\n",
    "    }\n",
    "    r = requests.get(endpoint, parameters, auth=(os.getenv(\"FROST_ID\"),''), headers= { \"User-Agent\": \"uionowciabs\" }) # random user agent\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        json = r.json()\n",
    "        for record in json[\"properties\"][\"timeseries\"]:\n",
    "            k = pd.to_datetime(record[\"time\"]).strftime(\"%Y-%m-%d\")\n",
    "            weather[airport_group][k] = {}\n",
    "\n",
    "            weather[airport_group][k][\"mean(air_temperature P1D)\"] = record[\"data\"][\"next_24_hours\"][\"details\"][\"air_temperature_mean\"]\n",
    "            weather[airport_group][k][\"sum(precipitation_amount P1D)\"] = record[\"data\"][\"next_24_hours\"][\"details\"][\"precipitation_amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f5df0",
   "metadata": {},
   "source": [
    "### Add weather forecast (closest to the prediction date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ca3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, row) in feat_full_pred.iterrows():\n",
    "    available_forecast_dates = list(enumerate(map(lambda v: pd.to_datetime(v), weather[row[\"airport_group\"]].keys())))\n",
    "    prediction_date = (len(available_forecast_dates), row[\"timestamp\"])\n",
    "\n",
    "    dates = available_forecast_dates + [prediction_date]\n",
    "    dates = sorted(dates, key=lambda v: v[1])\n",
    "    prediction_date_index = dates.index(prediction_date)\n",
    "\n",
    "    closest_available_forecast_index = -1\n",
    "    if prediction_date[1].hour >= 12 and prediction_date_index != len(dates) - 1:\n",
    "        closest_available_forecast_index = min(len(dates) - 1, prediction_date_index + 1)\n",
    "    else:\n",
    "        closest_available_forecast_index = max(0, prediction_date_index - 1)\n",
    "    \n",
    "    closest_available_forecast = dates[closest_available_forecast_index][1]\n",
    "    k = closest_available_forecast.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    for (forecast_k, forecast_v) in weather[row[\"airport_group\"]][k].items():\n",
    "        feat_full_pred.at[i, forecast_k] = forecast_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376beb9",
   "metadata": {},
   "source": [
    "# Combine datasets and split into training/validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eef58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = hourly.merge(feat_full, on=[\"airport_group\", \"timestamp\"], how=\"left\").sort_values(\"timestamp\")\n",
    "pred = hourly_pred.merge(feat_full_pred,on=[\"airport_group\",\"timestamp\"], how=\"left\").sort_values(\"timestamp\")\n",
    "\n",
    "dataset[\"timestamp\"] = pd.to_datetime(dataset[\"timestamp\"])\n",
    "pred[\"timestamp\"]    = pd.to_datetime(pred[\"timestamp\"])\n",
    "\n",
    "CUTOFF_VAL  = pd.Timestamp(\"2025-01-01\")  \n",
    "CUTOFF_TEST = pd.Timestamp(\"2025-04-01\")\n",
    "\n",
    "train = dataset[dataset[\"timestamp\"] <  CUTOFF_VAL].copy()\n",
    "val   = dataset[(dataset[\"timestamp\"] >= CUTOFF_VAL) & \n",
    "                (dataset[\"timestamp\"] <  CUTOFF_TEST)].copy()\n",
    "test  = dataset[dataset[\"timestamp\"] >= CUTOFF_TEST].copy()\n",
    "\n",
    "print(train.shape, val.shape, test.shape, pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49ef9f",
   "metadata": {},
   "source": [
    "# Export as `.csv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/processed_data/train.csv', index=False)\n",
    "val.to_csv('../data/processed_data/val.csv', index=False)\n",
    "test.to_csv('../data/processed_data/test.csv', index=False)\n",
    "pred.to_csv('../data/processed_data/predict_oct2025.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
